<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Overview — DataLabs</title>
    <link rel="stylesheet" href="styles.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=DM+Sans:ital,wght@0,400;0,500;0,600;0,700;1,400&family=Lora:ital,wght@0,400;0,500;0,600;0,700;1,400&display=swap" rel="stylesheet">
</head>
<body class="page-overview">
    <nav class="nav">
        <div class="nav-inner">
            <a href="index.html" class="logo">DataLabs</a>
            <div class="nav-links">
                <a href="index.html" class="nav-link">Home</a>
                <a href="overview.html" class="nav-link active">Overview</a>
            </div>
        </div>
    </nav>

    <main>
        <section class="section hero-section">
            <div class="wrap">
                <h1 class="hero-title">From raw video to machine-readable reality.</h1>
                <!-- The Problem -->
                <div class="block block-text">
                    <h3 class="block-heading">The Problem</h3>
                    <p>More video is not more insight. As AI moves from the digital screen to the physical world, we have hit a data wall. We are drowning in raw pixels but still lack fundamental, structured truth. In order to build high-level world models and teach machines to navigate our world, we must move beyond the limits of current data collection.</p>
                    <ul class="problem-list">
                        <li><strong>Raw Video is Unstructured Noise</strong>: AI models cannot be trained on raw as it is essentially just a stream of pixels. It lacks the granular, machine-readable structure AI needs to "understand" physical interactions.</li>
                        <li><strong>The Simulation Paradox</strong>: Simulated environments are bounded by their creators. They consistently fail to capture the entropy of the real world and these messy, unpredictable edge cases need to be carefully accounted for when building truly intelligent world models.</li>
                        <li><strong>Scaling Bottleneck</strong>: Human-in-the-loop labeling cannot keep pace with the exponential data demands of physical AI. It is too slow, too expensive, and physically impossible to scale to the petabytes required for world-class models.</li>
                    </ul>
                </div>
            </div>
        </section>
        <section class="section content-section">
            <div class="wrap">
                <!-- Our Solution -->
                <div class="block block-text">
                    <h3 class="block-heading">Our Solution</h3>
                    <p>DataLabs delivers Data-as-a-Service. We curate massive publicly available video datasets, like Creative Commons Youtube (approximately 49 million videos), run our AI pipeline to parse physical interactions and structure every frame, and output rich schemas including object tracking, actions, causal links, depth, and embeddings. One pipeline from public video to training-ready data.</p>
                </div>

                <!-- 3-step pipeline flow -->
                <div class="block">
                    <div class="solution-flow">
                        <div class="solution-step">
                            <h4>Video Acquisition</h4>
                            <p>We source millions of public videos within our legal framework spanning diverse real-world settings and use cases, depending on the needs and interests of our client labs.</p>
                        </div>
                        <span class="solution-arrow" aria-hidden="true"></span>
                        <div class="solution-step">
                            <h4>AI Pipeline</h4>
                            <p>Our pipeline parses each video into objects, actions, causal relationships, and temporal structure which is the fundamental training-ready structured data.</p>
                        </div>
                        <span class="solution-arrow" aria-hidden="true"></span>
                        <div class="solution-step">
                            <h4>Schema Generation</h4>
                            <p>Every frame becomes a machine-readable schema: visual, audio, physical, semantic. The labs then use these to teach behavior of the real-world.</p>
                        </div>
                    </div>
                </div>

                <!-- The AI Pipeline: accordion list of outputs -->
                <div class="block">
                    <h3 class="block-heading">The AI Pipeline</h3>
                    <p class="block-p">Our pipeline turns raw video into training-ready datasets at scale. Every run produces a full stack of structured signals, from legal and technical metadata through to pre-computed depth, causal graphs, and embeddings. Each dataset is designed for research and production use, with consistent schemas and provenance so you can train, evaluate, and deploy with confidence. We also customize the pipeline and outputted schemas depending on the requirements and specificities of the client lab. Our datasets include:</p>
                    <ul class="output-accordion" role="list">
                        <li class="output-accordion-item">
                            <button type="button" class="output-accordion-trigger" aria-expanded="true" aria-controls="out-actions" id="out-actions-btn">
                                <span class="output-accordion-title">Action Events</span>
                                <span class="output-accordion-icon" aria-hidden="true">↓</span>
                            </button>
                            <div class="output-accordion-panel" id="out-actions" role="region" aria-labelledby="out-actions-btn">
                                <div class="output-accordion-panel-inner">
                                    <p class="output-accordion-panel-desc">Who did what to whom, when, and with what result. Each event is an agent–action–object–target tuple with timestamps, so you can train on physical interactions instead of guessing from pixels. Useful for affordance learning, action recognition, and causal attribution.</p>
                                    <div class="output-accordion-panel-example">
                                        <pre class="output-accordion-example-code"><code><span class="hl-punctuation">{</span>
  <span class="hl-key">"event_id"</span><span class="hl-punctuation">: </span><span class="hl-string">"act_001"</span><span class="hl-punctuation">,</span>
  <span class="hl-key">"agent"</span><span class="hl-punctuation">: </span><span class="hl-string">"human_right_hand"</span><span class="hl-punctuation">,</span>
  <span class="hl-key">"action"</span><span class="hl-punctuation">: </span><span class="hl-string">"thermal_bridge_application"</span><span class="hl-punctuation">,</span>
  <span class="hl-key">"object"</span><span class="hl-punctuation">: </span><span class="hl-string">"obj_iron_01"</span><span class="hl-punctuation">,</span>
  <span class="hl-key">"target"</span><span class="hl-punctuation">: </span><span class="hl-string">"obj_pcb_01"</span><span class="hl-punctuation">,</span>
  <span class="hl-key">"start_ms"</span><span class="hl-punctuation">: </span><span class="hl-number">190400</span><span class="hl-punctuation">,</span>
  <span class="hl-key">"end_ms"</span><span class="hl-punctuation">: </span><span class="hl-number">198200</span>
<span class="hl-punctuation">}</span></code></pre>
                                    </div>
                                </div>
                            </div>
                        </li>
                        <li class="output-accordion-item">
                            <button type="button" class="output-accordion-trigger" aria-expanded="true" aria-controls="out-camera" id="out-camera-btn">
                                <span class="output-accordion-title">Camera Intrinsics</span>
                                <span class="output-accordion-icon" aria-hidden="true">↓</span>
                            </button>
                            <div class="output-accordion-panel" id="out-camera" role="region" aria-labelledby="out-camera-btn">
                                <div class="output-accordion-panel-inner">
                                    <p class="output-accordion-panel-desc">Lens and camera pose for every frame. You get focal length, principal point, and distortion so models can recover 3D geometry and fuse multiple views without guessing. Handy for metric reconstruction and any task that needs real scale.</p>
                                    <div class="output-accordion-panel-example">
                                        <pre class="output-accordion-example-code"><code><span class="hl-key">"intrinsics"</span><span class="hl-punctuation">: {</span>
  <span class="hl-key">"model"</span><span class="hl-punctuation">: </span><span class="hl-string">"pinhole"</span><span class="hl-punctuation">,</span>
  <span class="hl-key">"params"</span><span class="hl-punctuation">: { </span><span class="hl-key">"fx"</span><span class="hl-punctuation">: </span><span class="hl-number">1240.5</span><span class="hl-punctuation">, </span><span class="hl-key">"fy"</span><span class="hl-punctuation">: </span><span class="hl-number">1240.5</span><span class="hl-punctuation">,</span>
             <span class="hl-key">"cx"</span><span class="hl-punctuation">: </span><span class="hl-number">960</span><span class="hl-punctuation">, </span><span class="hl-key">"cy"</span><span class="hl-punctuation">: </span><span class="hl-number">540</span> <span class="hl-punctuation">}</span>
<span class="hl-punctuation">}</span></code></pre>
                                    </div>
                                </div>
                            </div>
                        </li>
                        <li class="output-accordion-item">
                            <button type="button" class="output-accordion-trigger" aria-expanded="true" aria-controls="out-causal" id="out-causal-btn">
                                <span class="output-accordion-title">Causal Graphs & DAGs</span>
                                <span class="output-accordion-icon" aria-hidden="true">↓</span>
                            </button>
                            <div class="output-accordion-panel" id="out-causal" role="region" aria-labelledby="out-causal-btn">
                                <div class="output-accordion-panel-inner">
                                    <p class="output-accordion-panel-desc">Events and states as nodes, cause–effect links as edges. The pipeline outputs a DAG so you can train models to answer “what if?” and plan, not just recognize. Edges can be necessity or sufficiency, so you know what must happen before what.</p>
                                    <div class="output-accordion-panel-example">
                                        <pre class="output-accordion-example-code"><code><span class="hl-key">"nodes"</span><span class="hl-punctuation">: [{ </span><span class="hl-key">"id"</span><span class="hl-punctuation">: </span><span class="hl-string">"N1"</span><span class="hl-punctuation">, </span><span class="hl-key">"type"</span><span class="hl-punctuation">: </span><span class="hl-string">"action"</span><span class="hl-punctuation">,</span>
  <span class="hl-key">"label"</span><span class="hl-punctuation">: </span><span class="hl-string">"Flux Application"</span> <span class="hl-punctuation">}],</span>
<span class="hl-key">"edges"</span><span class="hl-punctuation">: [{ </span><span class="hl-key">"from"</span><span class="hl-punctuation">: </span><span class="hl-string">"N1"</span><span class="hl-punctuation">, </span><span class="hl-key">"to"</span><span class="hl-punctuation">: </span><span class="hl-string">"N3"</span><span class="hl-punctuation">,</span>
  <span class="hl-key">"relation"</span><span class="hl-punctuation">: </span><span class="hl-string">"precondition"</span> <span class="hl-punctuation">}]</span></code></pre>
                                    </div>
                                </div>
                            </div>
                        </li>
                        <li class="output-accordion-item">
                            <button type="button" class="output-accordion-trigger" aria-expanded="true" aria-controls="out-cot" id="out-cot-btn">
                                <span class="output-accordion-title">Chain-of-thought & Counterfactuals</span>
                                <span class="output-accordion-icon" aria-hidden="true">↓</span>
                            </button>
                            <div class="output-accordion-panel" id="out-cot" role="region" aria-labelledby="out-cot-btn">
                                <div class="output-accordion-panel-inner">
                                    <p class="output-accordion-panel-desc">Step-by-step reasoning and what-if questions tied to the video. We annotate why things happen and what would change if something else had happened, so models learn to explain and handle edge cases. Good for interpretability and robust decision-making.</p>
                                    <div class="output-accordion-panel-example">
                                        <pre class="output-accordion-example-code"><code><span class="hl-punctuation">{</span>
  <span class="hl-key">"type"</span><span class="hl-punctuation">: </span><span class="hl-string">"counterfactual"</span><span class="hl-punctuation">,</span>
  <span class="hl-key">"question"</span><span class="hl-punctuation">: </span><span class="hl-string">"What if solder touched the iron first?"</span><span class="hl-punctuation">,</span>
  <span class="hl-key">"answer"</span><span class="hl-punctuation">: </span><span class="hl-string">"Would tin the tip but fail to wet the pad..."</span>
<span class="hl-punctuation">}</span></code></pre>
                                    </div>
                                </div>
                            </div>
                        </li>
                        <li class="output-accordion-item">
                            <button type="button" class="output-accordion-trigger" aria-expanded="true" aria-controls="out-manifest" id="out-manifest-btn">
                                <span class="output-accordion-title">Dataset Manifest & Provenance</span>
                                <span class="output-accordion-icon" aria-hidden="true">↓</span>
                            </button>
                            <div class="output-accordion-panel" id="out-manifest" role="region" aria-labelledby="out-manifest-btn">
                                <div class="output-accordion-panel-inner">
                                    <p class="output-accordion-panel-desc">One place for license, checksums, resolution, and scene context. You can verify provenance, rerun experiments, and filter by legal or technical criteria without digging through raw files. Everything you need for compliance and reproducibility is in one manifest.</p>
                                    <div class="output-accordion-panel-example">
                                        <pre class="output-accordion-example-code"><code><span class="hl-key">"admin_and_legal"</span><span class="hl-punctuation">: {</span>
  <span class="hl-key">"video_id"</span><span class="hl-punctuation">: </span><span class="hl-string">"skill.video_001"</span><span class="hl-punctuation">,</span>
  <span class="hl-key">"license_type"</span><span class="hl-punctuation">: </span><span class="hl-string">"CC-BY-4.0-Commercial"</span>
<span class="hl-punctuation">},</span>
<span class="hl-key">"technical_integrity"</span><span class="hl-punctuation">: {</span>
  <span class="hl-key">"temporal"</span><span class="hl-punctuation">: { </span><span class="hl-key">"fps"</span><span class="hl-punctuation">: </span><span class="hl-number">60</span> <span class="hl-punctuation">},</span>
  <span class="hl-key">"spatial"</span><span class="hl-punctuation">: { </span><span class="hl-key">"resolution"</span><span class="hl-punctuation">: [</span><span class="hl-number">1920</span><span class="hl-punctuation">, </span><span class="hl-number">1080</span><span class="hl-punctuation">] }</span>
<span class="hl-punctuation">}</span></code></pre>
                                    </div>
                                </div>
                            </div>
                        </li>
                        <li class="output-accordion-item">
                            <button type="button" class="output-accordion-trigger" aria-expanded="true" aria-controls="out-depth-flow" id="out-depth-flow-btn">
                                <span class="output-accordion-title">Metric Depth / Optical Flow</span>
                                <span class="output-accordion-icon" aria-hidden="true">↓</span>
                            </button>
                            <div class="output-accordion-panel" id="out-depth-flow" role="region" aria-labelledby="out-depth-flow-btn">
                                <div class="output-accordion-panel-inner">
                                    <p class="output-accordion-panel-desc">Metric depth (how far each pixel is) and optical flow (how it moves frame to frame), with confidence where needed. Real-world geometry and motion, no sim or hand labels. Depth is in meters; flow gives you dense motion for dynamics and temporal consistency.</p>
                                    <div class="output-accordion-panel-example">
                                        <pre class="output-accordion-example-code"><code><span class="hl-comment">// depth per pixel: [meters, confidence]</span>
<span class="hl-punctuation">[[</span><span class="hl-number">0.45</span><span class="hl-punctuation">, </span><span class="hl-number">0.98</span><span class="hl-punctuation">], [</span><span class="hl-number">0.12</span><span class="hl-punctuation">, </span><span class="hl-number">0.92</span><span class="hl-punctuation">]]</span>

<span class="hl-comment">// flow per frame</span>
<span class="hl-punctuation">{ </span><span class="hl-key">"mean_magnitude"</span><span class="hl-punctuation">: </span><span class="hl-number">1.25</span><span class="hl-punctuation">, </span><span class="hl-key">"flow_entropy"</span><span class="hl-punctuation">: </span><span class="hl-number">0.45</span> <span class="hl-punctuation">}</span></code></pre>
                                    </div>
                                </div>
                            </div>
                        </li>
                        <li class="output-accordion-item">
                            <button type="button" class="output-accordion-trigger" aria-expanded="true" aria-controls="out-panoptic" id="out-panoptic-btn">
                                <span class="output-accordion-title">Panoptic Masks / Object Tracking</span>
                                <span class="output-accordion-icon" aria-hidden="true">↓</span>
                            </button>
                            <div class="output-accordion-panel" id="out-panoptic" role="region" aria-labelledby="out-panoptic-btn">
                                <div class="output-accordion-panel-inner">
                                    <p class="output-accordion-panel-desc">Segmentation and tracking in one format: which pixels belong to which object, and the same ID across time. So models see who is where, when, and how they interact over the clip. Masks and bboxes at keyframes, with instance and semantic labels in a single representation.</p>
                                    <div class="output-accordion-panel-example">
                                        <pre class="output-accordion-example-code"><code><span class="hl-key">"dominant_objects"</span><span class="hl-punctuation">: [</span>
  <span class="hl-punctuation">{ </span><span class="hl-key">"id"</span><span class="hl-punctuation">: </span><span class="hl-string">"obj_iron_01"</span><span class="hl-punctuation">, </span><span class="hl-key">"label"</span><span class="hl-punctuation">: </span><span class="hl-string">"soldering_iron"</span> <span class="hl-punctuation">},</span>
  <span class="hl-punctuation">{ </span><span class="hl-key">"id"</span><span class="hl-punctuation">: </span><span class="hl-string">"obj_pcb_01"</span><span class="hl-punctuation">, </span><span class="hl-key">"label"</span><span class="hl-punctuation">: </span><span class="hl-string">"pcb"</span><span class="hl-punctuation">, </span><span class="hl-key">"static"</span><span class="hl-punctuation">: </span><span class="hl-bool">true</span> <span class="hl-punctuation">}</span>
<span class="hl-punctuation">],</span>
<span class="hl-key">"keyframes"</span><span class="hl-punctuation">: [{</span>
  <span class="hl-key">"timestamp_ms"</span><span class="hl-punctuation">: </span><span class="hl-number">190400</span><span class="hl-punctuation">,</span>
  <span class="hl-key">"objects"</span><span class="hl-punctuation">: [{ </span><span class="hl-key">"id"</span><span class="hl-punctuation">: </span><span class="hl-string">"obj_iron_01"</span><span class="hl-punctuation">,</span>
    <span class="hl-key">"bbox"</span><span class="hl-punctuation">: [</span><span class="hl-number">0.42</span><span class="hl-punctuation">, </span><span class="hl-number">0.48</span><span class="hl-punctuation">, </span><span class="hl-number">0.08</span><span class="hl-punctuation">, </span><span class="hl-number">0.12</span><span class="hl-punctuation">] }]</span>
<span class="hl-punctuation">}]</span></code></pre>
                                    </div>
                                </div>
                            </div>
                        </li>
                        <li class="output-accordion-item">
                            <button type="button" class="output-accordion-trigger" aria-expanded="true" aria-controls="out-task" id="out-task-btn">
                                <span class="output-accordion-title">Task Segmentation</span>
                                <span class="output-accordion-icon" aria-hidden="true">↓</span>
                            </button>
                            <div class="output-accordion-panel" id="out-task" role="region" aria-labelledby="out-task-btn">
                                <div class="output-accordion-panel-inner">
                                    <p class="output-accordion-panel-desc">Videos cut into segments by phase (e.g. setup, execution) with short summaries and milestones. Use it for goal-conditioned training or to benchmark “did the model get the procedure?” Each segment has start/end times and a complexity score so you can sample or weight by difficulty.</p>
                                    <div class="output-accordion-panel-example">
                                        <pre class="output-accordion-example-code"><code><span class="hl-punctuation">{</span>
  <span class="hl-key">"segment_id"</span><span class="hl-punctuation">: </span><span class="hl-string">"seg_001"</span><span class="hl-punctuation">,</span>
  <span class="hl-key">"type"</span><span class="hl-punctuation">: </span><span class="hl-string">"Preparation"</span><span class="hl-punctuation">,</span>
  <span class="hl-key">"task_phase"</span><span class="hl-punctuation">: </span><span class="hl-string">"Setup"</span><span class="hl-punctuation">,</span>
  <span class="hl-key">"summary"</span><span class="hl-punctuation">: </span><span class="hl-string">"Workspace organization and tool safety check."</span><span class="hl-punctuation">,</span>
  <span class="hl-key">"start_time_ms"</span><span class="hl-punctuation">: </span><span class="hl-number">0</span><span class="hl-punctuation">,</span>
  <span class="hl-key">"end_time_ms"</span><span class="hl-punctuation">: </span><span class="hl-number">45000</span>
<span class="hl-punctuation">}</span></code></pre>
                                    </div>
                                </div>
                            </div>
                        </li>
                        <li class="output-accordion-item">
                            <button type="button" class="output-accordion-trigger" aria-expanded="true" aria-controls="out-transcripts" id="out-transcripts-btn">
                                <span class="output-accordion-title">Transcripts & Entity Linking</span>
                                <span class="output-accordion-icon" aria-hidden="true">↓</span>
                            </button>
                            <div class="output-accordion-panel" id="out-transcripts" role="region" aria-labelledby="out-transcripts-btn">
                                <div class="output-accordion-panel-inner">
                                    <p class="output-accordion-panel-desc">Transcripts with timestamps, plus links from spoken words (e.g. “the tip”) to object IDs in the scene. Language and vision share the same vocabulary so you can train language-conditioned or audio–visual models. Optional acoustic events (clicks, clangs) are tagged where relevant.</p>
                                    <div class="output-accordion-panel-example">
                                        <pre class="output-accordion-example-code"><code><span class="hl-punctuation">{ </span><span class="hl-key">"utterance_id"</span><span class="hl-punctuation">: </span><span class="hl-string">"u_001"</span><span class="hl-punctuation">,</span>
  <span class="hl-key">"text"</span><span class="hl-punctuation">: </span><span class="hl-string">"Place the tip so it touches the pad and lead."</span><span class="hl-punctuation">,</span>
  <span class="hl-key">"start_ms"</span><span class="hl-punctuation">: </span><span class="hl-number">122000</span><span class="hl-punctuation">, </span><span class="hl-key">"end_ms"</span><span class="hl-punctuation">: </span><span class="hl-number">126500</span> <span class="hl-punctuation">},</span>
<span class="hl-key">"entities"</span><span class="hl-punctuation">: [</span>
  <span class="hl-punctuation">{ </span><span class="hl-key">"text"</span><span class="hl-punctuation">: </span><span class="hl-string">"tip"</span><span class="hl-punctuation">, </span><span class="hl-key">"object_id"</span><span class="hl-punctuation">: </span><span class="hl-string">"ID_001"</span> <span class="hl-punctuation">},</span>
  <span class="hl-punctuation">{ </span><span class="hl-key">"text"</span><span class="hl-punctuation">: </span><span class="hl-string">"pad"</span><span class="hl-punctuation">, </span><span class="hl-key">"object_id"</span><span class="hl-punctuation">: </span><span class="hl-string">"ID_002"</span> <span class="hl-punctuation">}</span>
<span class="hl-punctuation">]</span></code></pre>
                                    </div>
                                </div>
                            </div>
                        </li>
                        <li class="output-accordion-item">
                            <button type="button" class="output-accordion-trigger" aria-expanded="true" aria-controls="out-embeddings" id="out-embeddings-btn">
                                <span class="output-accordion-title">Visual & Acoustic Embeddings</span>
                                <span class="output-accordion-icon" aria-hidden="true">↓</span>
                            </button>
                            <div class="output-accordion-panel" id="out-embeddings" role="region" aria-labelledby="out-embeddings-btn">
                                <div class="output-accordion-panel-inner">
                                    <p class="output-accordion-panel-desc">Precomputed visual and audio vectors (e.g. SigLIP, CLAP) so you can skip feature extraction. Same pipeline, same schema—drop them into your trainer or retrieval stack and go. Visual is time-windowed (e.g. per second); audio is one vector per clip unless you ask otherwise.</p>
                                </div>
                            </div>
                        </li>
                    </ul>
                </div>

                <!-- Why DataLabs: bubble style -->
                <div class="block">
                    <h3 class="block-heading">Why DataLabs</h3>
                    <div class="why-bubbles">
                        <div class="why-bubble">
                            <span class="why-bubble-label">Training-Ready Data</span>
                            <p class="why-bubble-desc">Get structured schemas and annotations instead of raw video. Focus on training and evaluation instead of building preprocessing from scratch.</p>
                        </div>
                        <div class="why-bubble">
                            <span class="why-bubble-label">Depth & Physics</span>
                            <p class="why-bubble-desc">Per-frame depth, optical flow, physics, and camera intrinsics from real video. The kind of geometric signal simulators struggle to reproduce.</p>
                        </div>
                        <div class="why-bubble">
                            <span class="why-bubble-label">Singular Pipeline</span>
                            <p class="why-bubble-desc">One automated pipeline turns public video into training-ready datasets. No stitching together multiple vendors or tools.</p>
                        </div>
                        <div class="why-bubble">
                            <span class="why-bubble-label">Causal Reasoning</span>
                            <p class="why-bubble-desc">Our Causal Graphs and DAGs allow you to move beyond pattern matching toward reasoning and planning.</p>
                        </div>
                    </div>
                </div>
            </div>
        </section>
    </main>

    <footer class="footer">
        <div class="wrap footer-inner">
            <div class="footer-brand">
                <a href="index.html" class="logo">DataLabs</a>
                <p class="footer-tagline">Structured data for the physical world.</p>
            </div>
        </div>
    </footer>

    <script src="script.js"></script>
</body>
</html>
